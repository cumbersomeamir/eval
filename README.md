# eval
This repo is for pragmatic evaluation of LLMs

Contents:

1) Eval Dataset Generator
2) LLM Output Evaluator - 
IDEA
(i)aggregate the outputs of the llms when the rating have been generated (average)
(ii)Calculate the varaince of each model output iteration and try to minimize it
(iii)Try with different prompt engineering texts to find out which works better. 
(iv)Trt to limit the output generation or dataset such that it can be produced within one context window. 
